# Background Tasks with Celery

This project uses **Celery** with **Redis** as a message broker to run background tasks.  
Celery allows long-running or asynchronous operations to be processed **without blocking API requests**.

---

## ğŸ¯ Task API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| **POST** | `/api/tasks/process/` | Submit a task to Celery |
| **GET** | `/api/tasks/status/{task_id}/` | Get the status & result of a Celery task |

### ğŸ“ Example Requests:

#### **1ï¸âƒ£ Submit a Background Task**
```bash
curl -X POST http://localhost:8001/api/tasks/process/ -H "Content-Type: application/json" -d '{
  "data": "hello"
}'
```

âœ… **Expected Response:**
```json
{
    "task_id": "abc123"
}
```

---

#### **2ï¸âƒ£ Check Task Status**
```bash
curl -X GET http://localhost:8001/api/tasks/status/abc123/
```

âœ… **Possible Responses:**

ğŸ“Œ **Task is still processing:**
```json
{
    "task_id": "abc123",
    "status": "PENDING",
    "result": null
}
```

ğŸ“Œ **Task has completed successfully:**
```json
{
    "task_id": "abc123",
    "status": "SUCCESS",
    "result": {"processed_data": "HELLO"}
}
```

---

## âš™ï¸ How Celery Works

1ï¸âƒ£ A **task is sent to the Celery worker** via the API.  
2ï¸âƒ£ The **worker picks up the task from Redis** and starts processing it.  
3ï¸âƒ£ The client can **retrieve the task status** anytime using the task ID.  
4ï¸âƒ£ Once completed, **the task result is stored in Redis** and available via API.

---

## ğŸ”§ Configuring Celery in `settings.py`

Celery is **already pre-configured** inside `project/settings.py`:

```python
CELERY_BROKER_URL = 'redis://redis:6379/0'
CELERY_RESULT_BACKEND = 'redis://redis:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'
```

---

## ğŸš€ Running Celery with Docker

Celery is automatically started inside a **separate container** when using `docker-compose up`.

To **restart Celery manually**, run:
```bash
docker-compose restart celery
```

To **view Celery logs**, use:
```bash
docker-compose logs celery
```

---

## ğŸ”’ Best Practices for Background Tasks

âœ” **Use Celery for long-running tasks** (e.g., sending emails, processing files).  
âœ” **Avoid returning large task results** in the responseâ€”store them in the database instead.  
âœ” **Scale Celery workers** by running multiple instances (`celery -A djangoNinja worker --loglevel=info -c 4`).  

For API usage details, check **[API Documentation](api.md)**.
